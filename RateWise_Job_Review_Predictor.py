# -*- coding: utf-8 -*-
"""DefineAndSolveMLProblem_ (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YZ5OcxM8R-yNFzTIwF3Q_anq4aD583b4

# Lab 8: Define and Solve an ML Problem of Your Choosing
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

"""In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan.

You will complete the following tasks:

1. Build Your DataFrame
2. Define Your ML Problem
3. Perform exploratory data analysis to understand your data.
4. Define Your Project Plan
5. Implement Your Project Plan:
    * Prepare your data for your model.
    * Fit your model to the training data and evaluate your model.
    * Improve your model's performance.

## Part 1: Build Your DataFrame

You will have the option to choose one of four data sets that you have worked with in this program:

* The "census" data set that contains Census information from 1994: `censusData.csv`
* Airbnb NYC "listings" data set: `airbnbListingsData.csv`
* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`
* Book Review data set: `bookReviewsData.csv`

Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models.

#### Load a Data Set and Save it as a Pandas DataFrame

The code cell below contains filenames (path + filename) for each of the four data sets available to you.

<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`.

You can load each file as a new DataFrame to inspect the data before choosing your data set.
"""

df = pd.read_csv(r"C:\Users\aisha\OneDrive\Desktop\BTT AI\glassdoor_reviews.csv\glassdoor_reviews.csv")
df.head()

"""## Part 2: Define Your ML Problem

Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:

1. List the data set you have chosen.
2. What will you be predicting? What is the label?
3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?
4. What are your features? (note: this list may change after your explore your data)
5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?

The dataset I have chosen is the Glassdoor job reviews dataset, and I will be predicting the overall job rating, which is the label. This is a supervised learning problem and specifically a classification problem since the overall rating is a discrete value with categories ranging from 1 to 5. Features in the dataset include job title, location, work-life balance, company culture, and other review aspects. Predicting the overall rating is crucial for companies as it helps them enhance employee satisfaction, attract top talent, improve their reputation, and make strategic decisions that contribute to their competitive advantage.

## Part 3: Understand Your Data

The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:

1. What data preparation techniques would you like to use? These data preparation techniques may include:

    * addressing missingness, such as replacing missing values with means
    * finding and replacing outliers
    * renaming features and labels
    * finding and replacing outliers
    * performing feature engineering techniques such as one-hot encoding on categorical features
    * selecting appropriate features and removing irrelevant features
    * performing specific data cleaning and preprocessing techniques for an NLP problem
    * addressing class imbalance in your data sample to promote fair AI
    

2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?
    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?


3. How will you evaluate and improve the model's performance?
    * Are there specific evaluation metrics and methods that are appropriate for your model?
    

Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.

<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.

<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu.
"""

df['review_text'] = df[['headline', 'pros', 'cons']].fillna('').agg(' '.join, axis=1)

df.drop(columns=['headline', 'pros', 'cons'], inplace=True)

features = ['overall_rating', 'review_text', 'work_life_balance',
                        'culture_values', 'diversity_inclusion',
                        'career_opp', 'comp_benefits', 'senior_mgmt']

df = df[features]

print(df.dtypes)
print(df.shape)

df['overall_rating'] = pd.to_numeric(df['overall_rating'], errors='coerce')
df['work_life_balance'] = pd.to_numeric(df['work_life_balance'], errors='coerce')
df['culture_values'] = pd.to_numeric(df['culture_values'], errors='coerce')
df['diversity_inclusion'] = pd.to_numeric(df['diversity_inclusion'], errors='coerce')
df['career_opp'] = pd.to_numeric(df['career_opp'], errors='coerce')
df['comp_benefits'] = pd.to_numeric(df['comp_benefits'], errors='coerce')
df['senior_mgmt'] = pd.to_numeric(df['senior_mgmt'], errors='coerce')

df.replace([np.inf, -np.inf], np.nan, inplace=True)

df.dropna(subset=['overall_rating', 'work_life_balance', 'culture_values', 'diversity_inclusion', 'career_opp', 'comp_benefits', 'senior_mgmt'], inplace=True)

df['overall_rating'] = df['overall_rating'].astype(int)
df['work_life_balance'] = df['work_life_balance'].astype(int)
df['culture_values'] = df['culture_values'].astype(int)
df['diversity_inclusion'] = df['diversity_inclusion'].astype(int)
df['career_opp'] = df['career_opp'].astype(int)
df['comp_benefits'] = df['comp_benefits'].astype(int)
df['senior_mgmt'] = df['senior_mgmt'].astype(int)

# df['overall_rating'] = df['overall_rating'].fillna(df['overall_rating'].median()).astype(int)
# df['work_life_balance'] = df['work_life_balance'].fillna(df['work_life_balance'].median()).astype(int)
# df['culture_values'] = df['culture_values'].fillna(df['culture_values'].median()).astype(int)
# df['diversity_inclusion'] = df['diversity_inclusion'].fillna(df['diversity_inclusion'].median()).astype(int)
# df['career_opp'] = df['career_opp'].fillna(df['career_opp'].median()).astype(int)
# df['comp_benefits'] = df['comp_benefits'].fillna(df['comp_benefits'].median()).astype(int)
# df['senior_mgmt'] = df['senior_mgmt'].fillna(df['senior_mgmt'].median()).astype(int)

print(df.dtypes)
print(df.shape)

print(df.isnull().sum())

print(df.dtypes)

"""## Part 4: Define Your Project Plan

Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:

* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?
* Explain different data preparation techniques that you will use to prepare your data for modeling.
* What is your model (or models)?
* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data.

For this project, I will use the features: `review_text`, `overall_rating`, `work_life_balance`, `culture_values`, `diversity_inclusion`, `career_opp`, `comp_benefits`, and `senior_mgmt`. The review text will be processed with NLP techniques, including tokenization and TF-IDF vectorization, while numerical features will be used as is. I will implement a neural network with an input layer, three hidden layers, and an output layer with softmax activation to classify job ratings. The model will be trained using 75% of the data, with 20% of the training data reserved for validation, and evaluated on the test set. Performance will be assessed using accuracy and loss. Hyperparameters will be tuned and additional feature engineering explored to enhance model performance and generalization.

## Part 5: Implement Your Project Plan

<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#------------------------------------------------------------------#

# Natural Language Processing
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

#------------------------------------------------------------------#

# Machine Learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

#------------------------------------------------------------------#

# Model evaluation
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

#------------------------------------------------------------------#

#Neural Network
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import CategoricalCrossentropy
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""<b>Task:</b> Use the rest of this notebook to carry out your project plan.

You will:

1. Prepare your data for your model.
2. Fit your model to the training data and evaluate your model.
3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.

Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit.
"""

y = df['overall_rating']
X = df.drop(columns=['overall_rating'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

label_encoder = LabelEncoder()

y_train_encoded = label_encoder.fit_transform(y_train)

y_train_adjusted = y_train_encoded + 1

unique_adjusted_labels = np.unique(y_train_adjusted)
print(f"Unique adjusted labels: {unique_adjusted_labels}")

num_classes = len(unique_adjusted_labels)  # Should be 5
y_train = to_categorical(y_train_adjusted - 1, num_classes=num_classes)

y_test_encoded = label_encoder.transform(y_test)
y_test_adjusted = y_test_encoded + 1
y_test = to_categorical(y_test_adjusted - 1, num_classes=num_classes)

tfidf_vectorizer = TfidfVectorizer()

tfidf_vectorizer.fit(X_train['review_text'])

X_train_tfidf = tfidf_vectorizer.transform(X_train['review_text'])

X_test_tfidf = tfidf_vectorizer.transform(X_test['review_text'])

print('X_train_tfidf shape:', X_train_tfidf.shape)
print('X_test_tfidf shape:', X_test_tfidf.shape)

vocabulary_size = len(tfidf_vectorizer.vocabulary_)

print(vocabulary_size)

input_shape = X_train_tfidf.shape[1]

nn_model = Sequential()

nn_model.add(InputLayer(input_shape=(input_shape,)))

nn_model.add(Dense(128, activation='relu'))
nn_model.add(Dropout(0.5))

nn_model.add(Dense(64, activation='relu'))
nn_model.add(Dropout(0.5))

nn_model.add(Dense(32, activation='relu'))
nn_model.add(Dropout(0.5))

nn_model.add(Dense(5, activation='softmax'))

nn_model.summary()

sgd_optimizer = SGD(learning_rate=0.1)

loss_fn = CategoricalCrossentropy(from_logits=False)

nn_model.compile(optimizer=sgd_optimizer,
                 loss=loss_fn,
                 metrics=['accuracy'])

class ProgBarLoggerNEpochs(tf.keras.callbacks.Callback):
    def __init__(self, num_epochs: int, every_n: int = 50):
        super(ProgBarLoggerNEpochs, self).__init__()
        self.num_epochs = num_epochs
        self.every_n = every_n

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.every_n == 0:
            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)
            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)
                      for k, v in logs.items()]
            s_list = [s] + logs_s
            print(', '.join(s_list))

import time

t0 = time.time()

X_train_tfidf_array = X_train_tfidf.toarray()

num_epochs = 20

sample_size = 45000  # Adjust as necessary
X_train_tfidf_subset = X_train_tfidf_array[:sample_size]
y_train_subset = y_train[:sample_size]

history = nn_model.fit(
    X_train_tfidf_subset,
    y_train_subset,
    epochs=num_epochs,
    verbose=1,
    validation_split=0.2,
    callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)]
)

# Stop time
t1 = time.time()

# Print elapsed time
print('Elapsed time: %.2fs' % (t1 - t0))

history.history.keys()

# Plot training and validation loss
plt.plot(range(1, num_epochs + 1), history.history['loss'], label='Training Loss')
plt.plot(range(1, num_epochs + 1), history.history['val_loss'], label='Validation Loss')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


# Plot training and validation accuracy
plt.plot(range(1, num_epochs + 1), history.history['accuracy'], label='Training Accuracy')
plt.plot(range(1, num_epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(), y_test)

print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))

probability_predictions = nn_model.predict(X_test_tfidf.toarray())

y_test_array = y_test.argmax(axis=1)  # Convert one-hot encoded y_test back to labels

print("Predictions for the first 20 examples:")

for i in range(20):
    predicted_label = np.argmax(probability_predictions[i])
    actual_label = y_test_array[i]

    probability = probability_predictions[i][predicted_label]

    print(f"Example {i + 1}: Probability = {probability:.4f}, Predicted Label = {predicted_label + 1}, Actual Label = {actual_label + 1}")

num_reviews_to_display = 10

num_reviews_to_display = min(num_reviews_to_display, len(X_test))

review_texts = df.loc[X_test.index, 'review_text'].tolist()

for i in range(num_reviews_to_display):
    sample_index = i

 review_text = review_texts[sample_index]
    predicted_probabilities = probability_predictions[sample_index]
    predicted_rating = np.argmax(predicted_probabilities) + 1
    actual_rating = np.argmax(y_test[sample_index]) + 1

    print(f'Review #{sample_index + 1}:\n')
    print(review_text)
    print(f'\nPredicted Rating: {predicted_rating}\n')
    print(f'Actual Rating: {actual_rating}\n')
    print('-' * 80)

predicted_labels = np.argmax(probability_predictions, axis=1) + 1

actual_labels = np.argmax(y_test, axis=1) + 1  # Convert to 1-based index

conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=[1, 2, 3, 4, 5])

print("Confusion Matrix:")
print(conf_matrix)

predicted_labels = np.argmax(probability_predictions, axis=1) + 1

actual_labels = np.argmax(y_test, axis=1) + 1

conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=[1, 2, 3, 4, 5])

row_sums = conf_matrix.sum(axis=1, keepdims=True)
conf_matrix_percentage = conf_matrix.astype(float) / row_sums
conf_matrix_percentage = np.nan_to_num(conf_matrix_percentage)

plt.figure(figsize=(10, 8))
plt.imshow(conf_matrix_percentage, cmap='Blues', vmin=0, vmax=1)
plt.colorbar(label='Percentage')

for i in range(conf_matrix_percentage.shape[0]):
    for j in range(conf_matrix_percentage.shape[1]):
        plt.text(j, i, f'{conf_matrix_percentage[i, j]:.2f}',
                 ha='center', va='center', color='black', fontsize=12)

plt.xlabel('Predicted Rating', fontsize=14)
plt.ylabel('Actual Rating', fontsize=14)
plt.title('Confusion Matrix Percentage Heatmap', fontsize=16)
plt.xticks(ticks=np.arange(5), labels=[1, 2, 3, 4, 5], fontsize=12)
plt.yticks(ticks=np.arange(5), labels=[1, 2, 3, 4, 5], fontsize=12)
plt.grid(False)  # Disable gridlines in the plot
plt.show()

